{% set version = "3.4.5" %}
{% set nvshmem4py_version = "0.1.2.36470090" %}

{% set platform = "linux-x86_64" %}  # [linux64]
{% set platform = "linux-sbsa" %}    # [aarch64]
{% set extension = "tar.xz" %}

{% set soname = version.split(".")[0] %}
{% set cuda_major = environ.get("cuda_compiler_version", "13.0").split(".")[0] %}

{% set libnvshmem_sha256 = "058cbaddc4ff8646b8d1bd9322e93c90eae54c86e1ac8922f20d8a55a7fa8b7e" %}  # [linux64 and (cuda_compiler_version or "").startswith("12")]
{% set libnvshmem_sha256 = "18f12807c23bc5b270581af1f9bafd540369da783e557171558887623d77842c" %}  # [linux64 and (cuda_compiler_version or "").startswith("13")]
{% set libnvshmem_sha256 = "21c1efe6777dbfcbbc92444b420aab399f5b5590284ce09f1ccd8ec3516b192c" %}  # [aarch64 and (cuda_compiler_version or "").startswith("12")]
{% set libnvshmem_sha256 = "080e2ca6df463a5f3f5f50a3923ef5d491503837343649a922cb17718689fc06" %}  # [aarch64 and (cuda_compiler_version or "").startswith("13")]
{% set nvshmem_python_sha256 = "35292393b4370b23f177f0a1b549bde92931bf14bd66b0d54065ea11579e0b0b" %}  # [linux and (cuda_compiler_version or "").startswith(("12", "13"))]

package:
  name: libnvshmem-split
  version: {{ version }}

source:
  - url: https://developer.download.nvidia.com/compute/nvshmem/redist/libnvshmem/{{ platform }}/libnvshmem-{{ platform }}-{{ version }}_cuda{{ cuda_major }}-archive.{{ extension }}
    sha256: {{ libnvshmem_sha256 }}
  - url: https://developer.download.nvidia.com/compute/nvshmem/redist/nvshmem_python/source/nvshmem_python-source-{{ nvshmem4py_version }}_cuda12-archive.{{ extension }}
    folder: nvshmem4py
    sha256: {{ nvshmem_python_sha256 }}

build:
  number: 0
  skip: true  # [not (linux64 or aarch64)]
  skip: true  # [cuda_compiler_version in (None, "None") or (cuda_compiler_version or "").startswith("11")]

requirements:
  build:
    - cf-nvidia-tools 1  # [linux]

outputs:
  - name: libnvshmem{{ soname }}
    build:
      ignore_run_exports_from:
        - cuda-cudart-dev
      ignore_run_exports:
        - cuda-version
      missing_dso_whitelist:
        - "*libcuda.so*" # driver
        - "*openmpi*"    # omitted as dependency - customer HPC systems have own versions of MPI installed already
        - "*libfabric*"  # omitted as dependency - used for both Slingshot and EFA NICs. They have a custom libfabric installation and plugin
        - "*libfabric1*" # omitted as dependency - used for both Slingshot and EFA NICs. They have a custom libfabric installation and plugin
        - "*libmlx5*"    # omitted as dependency - needed for ibgda and ibdevx transport, installed as part of the Mellanox OFED
        - "*libmpi*"     # omitted as dependency - part of openmpi
        - "*libpmix*"    # omitted as dependency - part of openmpi
        - "*liboshmem*"  # omitted as dependency - part of openmpi
        - "*rdma-core*"  # omitted as dependency - MOFED replaces rdma-core
    files:
      - lib/*nvshmem*.so.*
      - lib/*nvshmem*.bc
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}
        - {{ stdlib("c") }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - cuda-cudart-dev
        - ucx
      run:
        - {{ pin_compatible("cuda-version", min_pin="x", max_pin="x") }}
        - cuda-cudart
        - libpciaccess
        - nccl
        - ucx
    test:
      commands:
        - test -L $PREFIX/lib/libnvshmem_host.so.{{ soname }}
        - test -f $PREFIX/lib/libnvshmem_host.so.{{ version }}
        - test -L $PREFIX/lib/nvshmem_bootstrap_mpi.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_mpi.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmi.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_pmi.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmi2.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_pmi2.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmix.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_pmix.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_bootstrap_shmem.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_shmem.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_bootstrap_uid.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_bootstrap_uid.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_transport_ibdevx.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_transport_ibdevx.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_transport_ibgda.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_transport_ibgda.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_transport_ibrc.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_transport_ibrc.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_transport_libfabric.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_transport_libfabric.so.{{ soname }}.0.0
        - test -L $PREFIX/lib/nvshmem_transport_ucx.so.{{ soname }}
        - test -f $PREFIX/lib/nvshmem_transport_ucx.so.{{ soname }}.0.0

  - name: libnvshmem-dev
    build:
      ignore_run_exports_from:
        - cuda-cudart-dev
      ignore_run_exports:
        - cuda-version
      missing_dso_whitelist:
        - "*libcuda.so*" # driver
        - "*libnvshmem_host.so.{{ soname }}" # avoids: ERROR (libnvshmem-dev,bin/perftest/device/pt-to-pt/shmem_put_bw): lib/libnvshmem_host.so.3 not found in any packages. We DO test for its existence though.
        - "*libmpi*"
        - "*libpmix*"
        - "*liboshmem*"
      run_exports:
        - {{ pin_subpackage("libnvshmem" ~ soname, max_pin=None) }}
    files:
      - bin/examples
      - bin/perftest
      - bin/nvshm*
      - bin/hydra_pmi_proxy
      - bin/hydra_nameserver
      - bin/hydra_persist
      - include/nvshmem*.h
      - include/bootstrap_device_host
      - include/device
      - include/device_host
      - include/device_host_transport
      - include/host
      - include/non_abi
      - lib/cmake/nvshmem
      - lib/*nvshmem*.so
      - share/src
    requirements:
      build:
        - {{ compiler("c") }}
        - {{ compiler('cuda') }}
        - {{ compiler("cxx") }}
        - {{ stdlib("c") }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - cuda-cudart-dev
        - libpciaccess  # [linux64 and (cuda_compiler_version or "").startswith("13")]
        - libhwloc      # [linux64 and (cuda_compiler_version or "").startswith("12")]
      run:
        - {{ pin_compatible("cuda-version", min_pin="x", max_pin="x") }}
        - {{ pin_subpackage("libnvshmem" ~ soname, exact=True) }}
        - cuda-cudart
    test:
      commands:
        - test -f $PREFIX/bin/hydra_pmi_proxy
        - test -f $PREFIX/bin/nvshmem-info
        - test -f $PREFIX/bin/nvshmrun.hydra
        - test -f $PREFIX/bin/hydra_nameserver
        - test -f $PREFIX/bin/nvshmrun
        - test -f $PREFIX/bin/hydra_persist
        - test -f $PREFIX/bin/examples/collective-launch
        - test -f $PREFIX/bin/examples/on-stream
        - test -f $PREFIX/bin/perftest/device/pt-to-pt/shmem_p_latency
        - test -f $PREFIX/bin/perftest/device/coll/reduction_latency
        - test -f $PREFIX/bin/perftest/host/init/malloc
        - test -f $PREFIX/bin/perftest/host/pt-to-pt/bw
        - test -f $PREFIX/bin/perftest/host/coll/broadcast_on_stream
        - test -f $PREFIX/include/bootstrap_device_host/nvshmem_uniqueid.h
        - test -f $PREFIX/include/device/nvshmem_defines.h
        - test -f $PREFIX/include/device_host/nvshmem_types.h
        - test -f $PREFIX/include/host/nvshmem_api.h
        - test -f $PREFIX/include/non_abi/device/coll/defines.cuh
        - test -f $PREFIX/include/device_host_transport/nvshmem_constants.h
        - test -f $PREFIX/include/nvshmem.h
        - test -f $PREFIX/include/nvshmemx.h
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMConfig.cmake
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMDeviceTargets-release.cmake
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMDeviceTargets.cmake
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMTargets-release.cmake
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMTargets.cmake
        - test -f $PREFIX/lib/cmake/nvshmem/NVSHMEMVersion.cmake
        - test -L $PREFIX/lib/libnvshmem_host.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_mpi.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmi.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmi2.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_pmix.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_shmem.so
        - test -L $PREFIX/lib/nvshmem_bootstrap_uid.so
        - test -L $PREFIX/lib/nvshmem_transport_ibdevx.so
        - test -L $PREFIX/lib/nvshmem_transport_ibgda.so
        - test -L $PREFIX/lib/nvshmem_transport_ibrc.so
        - test -L $PREFIX/lib/nvshmem_transport_libfabric.so
        - test -L $PREFIX/lib/nvshmem_transport_ucx.so

  - name: libnvshmem-static
    files:
      - lib/libnvshmem*.a
    requirements:
      run:
        - {{ pin_subpackage("libnvshmem-dev", exact=True) }}
    test:
      requires:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ stdlib("c") }}
        - cmake
        - make
        - cuda-version {{ cuda_compiler_version }}                     # [cuda_compiler_version != "None"]
        - {{ compiler('cuda') }}
      files:
        - compile_perf_test.sh
      commands:
        - test -f $PREFIX/lib/libnvshmem_device.a
        - test -f $PREFIX/lib/libnvshmem.a
        - bash compile_perf_test.sh

  - name: nvshmem4py
    version: {{ nvshmem4py_version }}
    build:
      ignore_run_exports_from:
        - {{ compiler("cxx") }}
        - cuda-cudart-dev
      ignore_run_exports:
        - cuda-version
      string: "py{{ py }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}"
      skip: true  # [py<310 or py>313]
      skip: true  # [cuda_compiler_version in (None, "None") or (cuda_compiler_version or "").startswith("11")]
      script: bash ${RECIPE_DIR}/nvshmem4py-build.sh
    requirements:
      build:
        - {{ compiler("c") }}
        - {{ compiler("cuda") }}
        - {{ compiler("cxx") }}
        - {{ stdlib("c") }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - cython >=0.29.24                          # needed by pyproject.toml
        - pip
        - python {{ python }}
        - setuptools >=62                           # needed by pyproject.toml
      run:
        - {{ pin_subpackage("libnvshmem-dev", exact=True) }}
        - cuda-core 0.3.2
        - cuda-python {{ cuda_major }}
        - numpy
    test:
      imports:
        - nvshmem
        - nvshmem.core
        - nvshmem.bindings
      commands:
        - |
          python - <<EOF
          import nvshmem, sys
          print("nvshmem.__version__ =", nvshmem.__version__)
          EOF
    about:
      home: https://docs.nvidia.com/nvshmem/index.html
      license: LicenseRef-NVIDIA-End-User-License-Agreement
      license_file: LICENSE
      license_url: https://docs.nvidia.com/nvshmem/api/sla.html
      summary: C++ accelerated Python bindings for NVIDIA NVSHMEM built with Cython
      description: |
        C++ accelerated Python bindings for NVIDIA NVSHMEM built with Cython. NVIDIA NVSHMEM is an NVIDIA based "shared memory" library that provides an easy-to-use CPU-side interface to allocate pinned memory that is symmetrically distributed across a cluster of NVIDIA GPUs.
      doc_url: https://docs.nvidia.com/nvshmem/api/index.html

about:
  home: https://docs.nvidia.com/nvshmem/index.html
  license: LicenseRef-NVIDIA-End-User-License-Agreement
  license_file: LICENSE
  license_url: https://docs.nvidia.com/nvshmem/api/sla.html
  summary: NVIDIA NVSHMEM is an NVIDIA based "shared memory" library that provides an easy-to-use CPU-side interface to allocate pinned memory that is symmetrically distributed across a cluster of NVIDIA GPUs.
  description: |
    NVIDIA NVSHMEM is an NVIDIA based "shared memory" library that provides an easy-to-use CPU-side interface to allocate pinned memory that is symmetrically distributed across a cluster of NVIDIA GPUs.
    NVSHMEM can significantly reduce communication and coordination overheads by allowing programmers to perform these operations from within CUDA kernels and on CUDA streams.
  doc_url: https://docs.nvidia.com/nvshmem/api/index.html

extra:
  feedstock-name: libnvshmem
  recipe-maintainers:
    - conda-forge/cuda
